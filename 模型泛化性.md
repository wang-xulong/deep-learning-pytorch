# 模型泛化性

##  泛化能力

指模型对潜在的/未知的数据的预测能力

P(x,y)很难得到，只能在训练集得到经验估计

大数定律，N很大时候，就逼近的真正的期望

## 泛化误差

影响泛化误差的两个因素：1）样本数N；2）假设条件D

1). 理论上可以用泛化误差进行评估，但是p(x,y)是未知的；转向使用模型在有限的数据集下、关于测试集的误差来进行经验估计，其合理性可以由大数定律解释。

 2). 如果函数复杂(假设条件多)，就有可能过拟合，只是在当前的数据集表现得很好。

## 泛化模型约束定理

泛化误差：损失函数在数据集上的期望。离散均分布就是所有数据点的损失函数加起来求平均。连续空间就是损失函数*局部x,y联合分布概率的积分。在联合分布概率未知的情况下，后者取得样本越多，越逼近前者。基于观测值得到的泛化误差和真实的泛化误差之间差的越小越好，而泛化约束定理说明了二者差值<ipsilon的概率不小于1-delta。ipsilon是关于delta、N、d的函数。N越大ipsilon越小, d（假设条件）越小，ipsilon越小，delta越大，ipsilon越小。也就是说，样本空间要大，假设条件要少。在此基础上ipsilon和概率成反向相关。证明过程就是要证明对于所有的拟合函数f，算出来的泛化误差和真实的泛化误差的差值（简称差值）<ipsilon的概率>= 1-delta, 也就是说要证存在一个f，差值>=ipsilon的概率<delta, 存在f其差值>=ipsilon的概率小于所有f的差值>=ipsilon的概率的总和(独立事件的性质)，代入hoeffding不等式得证。(yangzhang)



## Q&A

泛化模型约束定理所表达的含义是什么？

这个其实就是根据一些观测点去统计的平均值距离真实的期望值的距离，即中心极限定理。

你可以想象一个learning regression的问题，y都是在[0,1]之间的实数，然后loss是square error，这样你的模型在每个数据点上的误差就是在[0,1]里面。假如你训练数据很少，只看到了几个数据点的loss，那你的模型真实的generalization error就很难说清楚，反之，如果你看到了很多训练数据点上面的loss，那其实把这些loss做average，得到的empirical risk (loss)就很接近模型真实的generalization error了。(weinanzhang)